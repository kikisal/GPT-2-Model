This is a project on learning about the GPT model. I've misspelled it, but self attention is the best mechanism ever (as far as I know for now).
Using AdamW optimizer which i think it will also keep weights at ease and not blow too much thus not overfitting the model.
It is a Language Model, not yet a large one, trained on a simple dataset.
